\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amsfonts} % math (e.g. \text), blackboard symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography 
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Kian Kyars}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Formatting Tax: How Constraints Affect Reasoning}

\author{
  Kian Kyars \\
  Independent \\
  Edmonton \\
  December 2025 \\
  \texttt{kiankyars@gmail.com} \\
}


\begin{document}
\maketitle


\begin{abstract}
In this paper, I seek to answer the question of whether forcing a model to adhere to complex non-functional formatting rules degrades its ability to reason, by proxy of performance on a PhD-level reasoning benchmark. My objective is to provide actionable insights on the extent to which there exists a formatting tax on reasoning capabilities in AI agents, which will be useful for the engineering community. I use one of the current SoTA reasoning models, Claude Opus 4.5, on the Diamond GPQA benchmark, which is shown on the system card of all Frontier Lab models, to test how different reasoning constraints affect benchmark performance. My findings show that among the reasoning constraints I subject the model to, all degrade accuracy on the benchmark similarly, and that unconstrained Reasoning yields the best output
\end{abstract}


% keywords can be removed
\keywords{Reasoning \and Formatting \and GPQA \and CoT}


\section{Introduction}
Although it has been one year since the mainstream arrival of reasoning models, many aspects of their behavior are only understood weakly, and robust experimentation can strengthen our collective understanding. A better understanding on how prompting affects reasoning can help those using thinking models in their day-to-day workflow to better take advantage of them.

\section{Related Work}
Factory showed that context compression hurts agentic behavior; I focus on \emph{output} formatting \cite{factory}. GPQA Diamond is used in frontier model cards as a high bar for expert-level reasoning \cite{opus}.

\section{Methodology}

\subsection{Reasoning Models}
In this study, I test with Claude Opus 4.5, using the same parameters as the official Opus 4.5 GPQA model card results, which are located in Appendix~\ref{app:params}.

\subsection{Benchmark}
The Graduate-Level Google-Proof Q\&A benchmark (GPQA) is a set of very challenging
multiple-choice science questions. The GPQA Diamond subset of 198 questions are described by the developers of the test as the “highest quality subset which includes only questions where both experts answer correctly and the majority of non-experts answer incorrectly” \cite{rein2023gpqagraduatelevelgoogleproofqa}. Furthermore, if an "expert validator answers incorrectly … they [must] describe clearly the mistake or their understanding of the question writer’s explanation" \cite{rein2023gpqagraduatelevelgoogleproofqa}.

\subsection{Formatting Constraints}
Each condition uses the same task and answer rule: the outpput must contain \texttt{solution:X} with $X \in \{\text{A},\text{B},\text{C},\text{D}\}$. I add the following constraints on the \emph{reasoning}:

\begin{enumerate}
\item \textbf{Baseline (Prompt 0):} Identical to harness used in Opus 4.5 model card.

\item \textbf{Strict JSON (Prompt 1):} The model must output valid JSON only, containing exactly five keys: \texttt{initial\_intuition}, \texttt{step\_by\_step\_logic}, \texttt{potential\_counterarguments}, \texttt{confidence\_score\_0\_to\_1}, and \texttt{solution}.

\item \textbf{Structural Rigidity (Prompt 2):} Reasoning must consist of exactly three bullet points, each no longer than 20 words, and must not use the words "because" or "therefore".

\item \textbf{Python Code (Prompt 3):} The model must write its reasoning in Python.

\item \textbf{Oulipo Constraint (Prompt 4):} The letter 'e' cannot appear anywhere in the reasoning chain, based on Oulipo.

\item \textbf{Restricted Vocabulary (Prompt 5):} Reasoning cannot use 16 specific high-norm English tokens identified from GPT-OSS 120B embeddings: accordingly, code, ocode, The, settings, Moreover, description, Let's, This, core, utilizes, revolves, Here's, possibly, logic, and thereby \cite{finke}.
\end{enumerate}

\section{Experimental Setup}
I evaluate each formatting constraint on the full GPQA Diamond dataset (198 questions) with 5 repetitions per question-constraint pair, resulting in 990 total evaluations per condition. Questions are presented with randomly shuffled answer choices to prevent position bias. All experiments use Claude Opus 4.5 with identical parameters (see Appendix~\ref{app:params}).

Accuracy is calculated as the fraction of correct answers per condition, aggregated across all questions and repetitions.

\section{Results}

\subsection{Accuracy by Formatting Constraint}
Table~\ref{tab:prompt_summary} presents accuracy results. Anthropic reports 87.0\% on GPQA Diamond in their official model card \cite{opus}; our Baseline (87.2\%) is consistent with that result.

\begin{table}[h]
\centering
\input{../output/summary_table.tex}
\end{table}

\subsection{Token Usage Analysis}
Figure~\ref{fig:token_usage} in Appendix~\ref{app:figures} shows token usage patterns across conditions. Structural Rigidity yields the feweest output tokens, consistent with the 20-word-per-bullet and three-bullet restriction.

\subsection{Error Analysis}
I measure format compliance with prompt-specific checks (e.g., parseable JSON for Strict JSON; absence of `e' for Oulipo; see \texttt{test.py}). Table~\ref{tab:compliance} reports the fraction of responses that followed each constraint. Full outputs and violation records are in Appendix~\ref{app:data}.

\begin{table}[h]
\centering
\input{../output/compliance_table.tex}
\end{table}

\section{Limitations}
This study uses a single model (Claude Opus 4.5) and one benchmark (GPQA Diamond). Generalization to other reasoning models and benchmarks is unknown.

\section{Conclusion}
What I conclude from this study is that unconstrained formatting gives higher accuracy, as expected. There does not seem to be a specific format constraint which lobotomizes the model more than others, among those I tested.

\section*{Acknowledgments}
I thought of this idea after reading an article by Factory on the importance of context engineering for agentic performance and an article by Lennart Finke, this paper marries ideas from both \cite{factory,finke}.

\appendix
\section{Figures}
\label{app:figures}

\begin{figure}[h]
\centering
\IfFileExists{../output/token_usage.pdf}{%
  \includegraphics[width=0.8\textwidth]{../output/token_usage.pdf}%
}{\fbox{\parbox{0.6\textwidth}{\centering\small [token\_usage.pdf]}}}
\caption{Token usage by formatting constraint}
\label{fig:token_usage}
\end{figure}

\section{Model and API Parameters}
\label{app:params}
I use the Messages API with: \texttt{model=claude-opus-4-5-20251101}; \texttt{thinking=\{type: enabled, budget\_tokens: 64000\}}; \texttt{output\_config.effort=high}; \texttt{max\_tokens=64000}. Betas: \texttt{interleaved-thinking-2025-05-14}, \texttt{effort-2025-11-24}. This matches the setup used for GPQA in the Opus 4.5 system card \cite{opus}.

\section{Accuracy by Subdomain and Prompt}
\label{app:subdomain_prompt}
\input{../output/accuracy_subdomain_prompt.tex}

\section{Data and Reproducibility}
\label{app:data}
The Hugging Face dataset \url{https://huggingface.co/datasets/kyars/gpqa-results} contains the full 5,940 outputs (198 questions $\times$ 5 repetitions $\times$ 6 conditions) and the specific entries that violated each constraint (\texttt{violations\_by\_constraint}).

\section{Prompts and Reproducibility}
\label{app:prompts}
Code and prompts are available at \url{https://github.com/kiankyars/gpqa}. The six prompt variants (Baseline, Strict JSON, Structural Rigidity, Python Code, Oulipo, Restricted Vocabulary) are defined in \texttt{main.py}.

\begin{figure}[h]
  \centering
  \IfFileExists{../output/accuracy_with_error_bars.pdf}{%
    \includegraphics[width=0.8\textwidth]{../output/accuracy_with_error_bars.pdf}%
  }{\fbox{\parbox{0.6\textwidth}{\centering\small [accuracy\_with\_error\_bars.pdf]}}}
  \caption{Accuracy with 95\% intervals by formatting constraint}
  \label{fig:accuracy_bars}
  \end{figure}

%Bibliography
\bibliographystyle{plain}
\bibliography{references}  


\end{document}
